{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96943622-7d32-4d73-aa31-ad9059ad9bbe",
   "metadata": {},
   "source": [
    "8. Deep Learning.\n",
    "   \n",
    "a. Build a DNN with five hidden layers of 100 neurons each, He initialization,\n",
    "and the ELU activation function.\n",
    "\n",
    "b. Using Adam optimization and early stopping, try training it on MNIST but\n",
    "only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the\n",
    "next exercise. You will need a softmax output layer with five neurons, and as\n",
    "always make sure to save checkpoints at regular intervals and save the final\n",
    "model so you can reuse it later.\n",
    "\n",
    "c. Tune the hyperparameters using cross-validation and see what precision you\n",
    "can achieve.\n",
    "\n",
    "d. Now try adding Batch Normalization and compare the learning curves: is it\n",
    "converging faster than before? Does it produce a better model?\n",
    "\n",
    "e. Is the model overfitting the training set? Try adding dropout to every layer\n",
    "and try again. Does it help?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f115cba-5865-47e7-9e87-a6a6a715cb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40cbf68a-066e-4f7e-9b96-dcd48033dd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the MNIST DATA\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test_full, y_test_full) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09c8e46-3df6-4c52-bd90-d78780575a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(60000,)\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# That has the whole datset with features 0 to 9, now we have to split the dataset by\n",
    "# the features from 0 to 4, 5 to 9  so let's see how to do that\n",
    "\n",
    "# Check what your're working with\n",
    "print(type(X_train_full)) \n",
    "print(type(y_train_full))\n",
    "print(y_train_full.shape)\n",
    "print(X_train_full.shape)\n",
    "#print(X_train_full[0, :])\n",
    "#print(y_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a460e9e-809a-4240-a88c-45e27aa8837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30596, 28, 28)\n",
      "(29404, 28, 28)\n",
      "(30596,)\n",
      "(29404,)\n"
     ]
    }
   ],
   "source": [
    "# Okay, its a numpy array so find how to split it \n",
    "\n",
    "def split_data_by_labels(x_data, y_data):\n",
    "    # create an empty list for the new datas\n",
    "    X_split_04 = []\n",
    "    X_split_59 = []\n",
    "    y_split_04 = []\n",
    "    y_split_59 = []\n",
    "\n",
    "    # Split by the labels\n",
    "    for i, j in zip(x_data, y_data):\n",
    "        if j in [0, 1, 2, 3, 4]:\n",
    "            X_split_04.append(i)\n",
    "            y_split_04.append(j)\n",
    "        else:\n",
    "            X_split_59.append(i)\n",
    "            y_split_59.append(j)\n",
    "\n",
    "    # change back to numpy array\n",
    "    X_split_04 = np.array(X_split_04)\n",
    "    X_split_59 = np.array(X_split_59)\n",
    "    y_split_04 = np.array(y_split_04)\n",
    "    y_split_59 = np.array(y_split_59)\n",
    "\n",
    "    return X_split_04, X_split_59, y_split_04, y_split_59\n",
    "\n",
    "# Split Test and train data\n",
    "X_train_04, X_train_59, y_train_04, y_train_59 = split_data_by_labels(X_train_full, y_train_full)\n",
    "X_test_04, X_test_59, y_test_04, y_test_59 = split_data_by_labels(X_test_full, y_test_full)\n",
    "\n",
    "# Check\n",
    "print(X_train_04.shape)\n",
    "print(X_train_59.shape)\n",
    "print(y_train_04.shape)\n",
    "print(y_train_59.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b2ae444-d7c6-4a1b-8077-6d2298c0da23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27536, 28, 28)\n",
      "(3060, 28, 28)\n",
      "(26463,)\n",
      "(2941,)\n"
     ]
    }
   ],
   "source": [
    "# Split train set into training and validation sets\n",
    "X_train_04, X_val_04, y_train_04, y_val_04 = train_test_split(X_train_04, y_train_04, test_size=0.1)\n",
    "X_train_59, X_val_59, y_train_59, y_val_59 = train_test_split(X_train_59, y_train_59, test_size=0.1)\n",
    "\n",
    "# Check\n",
    "print(X_train_04.shape)\n",
    "print(X_val_04.shape)\n",
    "print(y_train_59.shape)\n",
    "print(y_val_59.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9367d9ce-9f2f-4699-a3b7-3d5700d94db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing step\n",
    "# Normalize the images to be between 0 to 1\n",
    "X_train_04 = X_train_04 / 255.0\n",
    "X_val_04 = X_val_04 / 255.0\n",
    "X_test_04 = X_test_04 / 255.0\n",
    "X_train_59 = X_train_59 / 255.0\n",
    "X_val_59 = X_val_59 / 255.0\n",
    "X_test_59 = X_test_59 / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4d729c-2f17-49e4-bd6b-dc15f464511d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Evidence\\PycharmProjects\\My-ML-Lab\\venv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Okay so starting with 0 to 4 to build the base model\n",
    "\n",
    "# Let's build a sequential model cause...yeah\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]), # Input layer, flatten because MNIST\n",
    "    # 5 hidden layers\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(5, activation=\"softmax\") # Since we are training from 0 to 4 of the MNIST data, we are using 5 output neurons, then softmax for activation good for classification\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cf51595-cd67-42e0-bb13-987d2d80710b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">78,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">505</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m78,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m505\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">119,405</span> (466.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m119,405\u001b[0m (466.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">119,405</span> (466.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m119,405\u001b[0m (466.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the details of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa70ae4-560c-427d-bf8e-ae4013196957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b49a669-f4b4-464b-b3b5-b9c21fd298d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - accuracy: 0.9349 - loss: 0.1912 - val_accuracy: 0.9797 - val_loss: 0.0659\n",
      "Epoch 2/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9829 - loss: 0.0530 - val_accuracy: 0.9791 - val_loss: 0.0673\n",
      "Epoch 3/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9894 - loss: 0.0328 - val_accuracy: 0.9843 - val_loss: 0.0469\n",
      "Epoch 4/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9900 - loss: 0.0323 - val_accuracy: 0.9879 - val_loss: 0.0426\n",
      "Epoch 5/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9925 - loss: 0.0235 - val_accuracy: 0.9837 - val_loss: 0.0583\n",
      "Epoch 6/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9914 - loss: 0.0251 - val_accuracy: 0.9892 - val_loss: 0.0360\n",
      "Epoch 7/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9945 - loss: 0.0170 - val_accuracy: 0.9866 - val_loss: 0.0480\n",
      "Epoch 8/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9945 - loss: 0.0184 - val_accuracy: 0.9899 - val_loss: 0.0388\n",
      "Epoch 9/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9953 - loss: 0.0130 - val_accuracy: 0.9905 - val_loss: 0.0428\n",
      "Epoch 10/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9968 - loss: 0.0098 - val_accuracy: 0.9876 - val_loss: 0.0560\n",
      "Epoch 11/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9959 - loss: 0.0124 - val_accuracy: 0.9902 - val_loss: 0.0423\n",
      "Epoch 12/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9959 - loss: 0.0136 - val_accuracy: 0.9889 - val_loss: 0.0482\n",
      "Epoch 13/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9966 - loss: 0.0110 - val_accuracy: 0.9873 - val_loss: 0.0525\n",
      "Epoch 14/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9963 - loss: 0.0149 - val_accuracy: 0.9882 - val_loss: 0.0518\n",
      "Epoch 15/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - accuracy: 0.9975 - loss: 0.0082 - val_accuracy: 0.9915 - val_loss: 0.0371\n",
      "Epoch 16/100\n",
      "\u001b[1m861/861\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9965 - loss: 0.0138 - val_accuracy: 0.9873 - val_loss: 0.0385\n"
     ]
    }
   ],
   "source": [
    "# Set up the callback that you will be using to fit\n",
    "# Here we are using EarlyStopping and ModelCheckpoint\n",
    "my_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True), # so that when it stops improving after 10 epochs, it would stop\n",
    "    keras.callbacks.ModelCheckpoint(\"mnist_04_bm.keras\", save_best_only=True), # this saves the model at regular intervals but at the epochs with best results\n",
    "]\n",
    "    \n",
    "\n",
    "# Now fit the model\n",
    "history = model.fit(x=X_train_04, y=y_train_04, epochs=100, validation_data=(X_val_04, y_val_04), callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4619ca32-53db-488b-863a-9cefae1094c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('mnist_04_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b6b8f-ffa4-4d6b-ad35-7bf45b8392fa",
   "metadata": {},
   "source": [
    "Tune the hyperparameters using cross-validation and see what precision you\n",
    "can achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d509e6d5-d02d-4b57-91db-c3810059dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is giving me a bit of a pause.\n",
    "# a lot of things have beem deprecated from keras so idk what to use\n",
    "# I'm going to try randomized search cv and hope it works\n",
    "\n",
    "# First define a function to build a model, idk i already have a model so wont it be like creating a new model?\n",
    "def build_model(learning_rate=1e-2):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(input_shape=[28,28]), # Input layer, flatten because MNIST\n",
    "        # 5 hidden layers\n",
    "        keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "        keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "        keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "        keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "        keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "        keras.layers.Dense(5, activation=\"softmax\") # Since we are training from 0 to 4 of the MNIST data, we are using 5 output neurons, then softmax for activation good for classification\n",
    "    ])\n",
    "\n",
    "    # Compile the model with a specified learning rate\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18e92d41-e295-4278-b824-af49047fbba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">78,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">505</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m78,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,100\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m505\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">119,405</span> (466.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m119,405\u001b[0m (466.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">119,405</span> (466.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m119,405\u001b[0m (466.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8fe086-5332-428e-a5df-d2bd3a3b9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "my_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True), # so that when it stops improving after 10 epochs, it would stop\n",
    "    keras.callbacks.ModelCheckpoint(\"mnist_04_tm.keras\", save_best_only=True), # this saves the model at regular intervals but at the epochs with best results\n",
    "]\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": np.logspace(-4, -1, 4), # Learning rate between 3e-4 and 3e-2\n",
    "    \"batch_size\": np.arange(16, 65, 16)\n",
    "}\n",
    "\n",
    "# Initialize KFold for cross-validation\n",
    "kf=KFold(n_splits=3)\n",
    "\n",
    "# Store the results of hyperparameter tuning\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3d2f273-f72c-493e-aecc-f74f1e7fc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(set(y_train_04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c34e90c-a7cd-404f-9bf9-fd1179b557b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train_04.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d608d16-9f71-4062-bbb8-04c93468d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_train_04.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f08dd-9526-455c-9168-aacd5fb41008",
   "metadata": {},
   "source": [
    "**Best Hyperparameters: {'learning_rate': np.float64(0.1), 'batch_size': np.int64(16), 'avg_val_score': np.float64(0.7426687677701315)}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7e19193-3bf4-40b9-a6bf-833e9e32bebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'learning_rate': np.float64(0.0001), 'batch_size': np.int64(16), 'avg_val_score': np.float64(0.6724885640045007)}, {'learning_rate': np.float64(0.0001), 'batch_size': np.int64(32), 'avg_val_score': np.float64(0.6722778417170048)}, {'learning_rate': np.float64(0.0001), 'batch_size': np.int64(48), 'avg_val_score': np.float64(0.6721367090940475)}, {'learning_rate': np.float64(0.0001), 'batch_size': np.int64(64), 'avg_val_score': np.float64(0.6721557701627413)}, {'learning_rate': np.float64(0.001), 'batch_size': np.int64(16), 'avg_val_score': np.float64(0.6743382774293423)}, {'learning_rate': np.float64(0.001), 'batch_size': np.int64(32), 'avg_val_score': np.float64(0.6721340281267961)}, {'learning_rate': np.float64(0.001), 'batch_size': np.int64(48), 'avg_val_score': np.float64(0.6737084810932478)}, {'learning_rate': np.float64(0.001), 'batch_size': np.int64(64), 'avg_val_score': np.float64(0.6732814150551955)}, {'learning_rate': np.float64(0.01), 'batch_size': np.int64(16), 'avg_val_score': np.float64(0.6904146571954092)}, {'learning_rate': np.float64(0.01), 'batch_size': np.int64(32), 'avg_val_score': np.float64(0.6798089121778806)}, {'learning_rate': np.float64(0.01), 'batch_size': np.int64(48), 'avg_val_score': np.float64(0.6798444216450056)}, {'learning_rate': np.float64(0.01), 'batch_size': np.int64(64), 'avg_val_score': np.float64(0.6789611751834551)}, {'learning_rate': np.float64(0.1), 'batch_size': np.int64(16), 'avg_val_score': np.float64(0.7426687677701315)}, {'learning_rate': np.float64(0.1), 'batch_size': np.int64(32), 'avg_val_score': np.float64(0.6707061330477396)}, {'learning_rate': np.float64(0.1), 'batch_size': np.int64(48), 'avg_val_score': np.float64(0.7008503278096517)}, {'learning_rate': np.float64(0.1), 'batch_size': np.int64(64), 'avg_val_score': np.float64(0.6866742769877116)}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549fed0-b144-4e88-8d53-6f60919b8eef",
   "metadata": {},
   "source": [
    "Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97b9b1fd-b269-447d-8eff-4d6933b0662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I didnt add the Tensorboard in the model before so idk how to check the learning curve now\n",
    "# and i am not going back it took too long to run\n",
    "# where does time go :(\n",
    "\n",
    "# On with BatchNormalization\n",
    "model = keras.models.Sequential([\n",
    "    # Flatten input images\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "\n",
    "    # BN before activation\n",
    "    keras.layers.BatchNormalization(), \n",
    "\n",
    "    # Dense layer with no bias term (because BN includes bias)\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "\n",
    "    # Activation function before BN\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "\n",
    "    # BN again before Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "\n",
    "    # BN again before Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "\n",
    "    # BN again before Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "\n",
    "    # BN again before Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "\n",
    "    # Final BN and output layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(5, activation=\"softmax\")\n",
    "\n",
    "])\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "760b3690-6d6d-4003-9091-cc27acef441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a specified learning rate from CV\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.1)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3475923d-7f99-4b3a-bcd9-b2e5d3d0ca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - accuracy: 0.8394 - loss: 0.6040 - val_accuracy: 0.9350 - val_loss: 0.2582\n",
      "Epoch 2/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.9186 - loss: 0.3688 - val_accuracy: 0.9042 - val_loss: 355294.5000\n",
      "Epoch 3/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9271 - loss: 0.3191 - val_accuracy: 0.9304 - val_loss: 913260352.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9521 - loss: 0.1840 - val_accuracy: 0.9654 - val_loss: 270036512.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9431 - loss: 0.2967 - val_accuracy: 0.9683 - val_loss: 306993760.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9547 - loss: 0.2027 - val_accuracy: 0.9683 - val_loss: 202627842048.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9631 - loss: 0.1670 - val_accuracy: 0.5392 - val_loss: 131530293248.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.9479 - loss: 0.4194 - val_accuracy: 0.9592 - val_loss: 14034470436864.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.9615 - loss: 0.1736 - val_accuracy: 0.9605 - val_loss: 6469007360.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 7ms/step - accuracy: 0.9598 - loss: 0.1654 - val_accuracy: 0.8696 - val_loss: 60099427041280.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6ms/step - accuracy: 0.9574 - loss: 0.2795 - val_accuracy: 0.9160 - val_loss: 750015938560.0000\n"
     ]
    }
   ],
   "source": [
    "my_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True), # so that when it stops improving after 10 epochs, it would stop\n",
    "    keras.callbacks.ModelCheckpoint(\"mnist_04_BN.keras\", save_best_only=True), # this saves the model at regular intervals but at the epochs with best results\n",
    "    keras.callbacks.TensorBoard(\"logs\")\n",
    "]\n",
    "    \n",
    "\n",
    "# Now fit the model\n",
    "history = model.fit(x=X_train_04, y=y_train_04, epochs=100, validation_data=(X_val_04, y_val_04), callbacks=my_callbacks, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68a2c4a0-e447-45f4-87bd-9834e5cab647",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9d7577d-157f-4468-a408-13178eed7ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5da70a9390f22d88\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5da70a9390f22d88\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3011df-2d56-4149-83b9-438db7d173a9",
   "metadata": {},
   "source": [
    "e. Is the model overfitting the training set? Try adding dropout to every layer\n",
    "and try again. Does it help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "657eacc7-923e-47e6-9748-c011f0a32263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Dropout to the model\n",
    "model = keras.models.Sequential([\n",
    "    # Flatten input images\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "\n",
    "    # BN before activation\n",
    "    keras.layers.BatchNormalization(), \n",
    "\n",
    "    # Apply a dropout rate of 20%\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "\n",
    "    # Dense layer with no bias term (because BN includes bias)\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "\n",
    "    # Activation function before BN\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "\n",
    "    # BN again before Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "\n",
    "    # BN again before Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "\n",
    "    # BN again before Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "\n",
    "    # BN again before Dense layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "\n",
    "    # Final BN and output layer\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(5, activation=\"softmax\")\n",
    "\n",
    "])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "709ab436-39d5-4cab-acb1-ea945f07776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a specified learning rate from CV\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.1)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "274c9ba1-d83f-411e-acc5-a022efb756a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7840 - loss: 0.8519 - val_accuracy: 0.9529 - val_loss: 0.3533\n",
      "Epoch 2/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8536 - loss: 0.6266 - val_accuracy: 0.9637 - val_loss: 0.2482\n",
      "Epoch 3/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8831 - loss: 0.5155 - val_accuracy: 0.9634 - val_loss: 5.8047\n",
      "Epoch 4/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.9054 - loss: 0.3858 - val_accuracy: 0.9585 - val_loss: 15891.5703\n",
      "Epoch 5/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 7ms/step - accuracy: 0.8937 - loss: 0.4785 - val_accuracy: 0.9670 - val_loss: 6.7902\n",
      "Epoch 6/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9059 - loss: 0.4513 - val_accuracy: 0.9686 - val_loss: 14501.1279\n",
      "Epoch 7/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.8979 - loss: 0.5743 - val_accuracy: 0.9725 - val_loss: 37954600.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9142 - loss: 0.5328 - val_accuracy: 0.9742 - val_loss: 23627.1172\n",
      "Epoch 9/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.9250 - loss: 0.3765 - val_accuracy: 0.9402 - val_loss: 10374.0615\n",
      "Epoch 10/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9160 - loss: 0.6045 - val_accuracy: 0.9755 - val_loss: 432578.1250\n",
      "Epoch 11/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.9335 - loss: 0.2919 - val_accuracy: 0.9608 - val_loss: 967495296.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m1721/1721\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - accuracy: 0.9121 - loss: 0.4932 - val_accuracy: 0.9745 - val_loss: 833040.6250\n"
     ]
    }
   ],
   "source": [
    "my_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True), # so that when it stops improving after 10 epochs, it would stop\n",
    "    keras.callbacks.ModelCheckpoint(\"mnist_04_DO.keras\", save_best_only=True), # this saves the model at regular intervals but at the epochs with best results\n",
    "    keras.callbacks.TensorBoard(\"logs/fit/do\")\n",
    "]\n",
    "    \n",
    "\n",
    "# Now fit the model\n",
    "history = model.fit(x=X_train_04, y=y_train_04, epochs=100, validation_data=(X_val_04, y_val_04), callbacks=my_callbacks, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92bef752-916b-491c-a16f-b4336fd2ebcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a3ac2a5202c91807\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a3ac2a5202c91807\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs/fit/do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "740f743f-0f74-45ab-b022-ed5e7e66d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be using this as the base model\n",
    "# save the model\n",
    "model.save('mnist_04_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744dd4bd-2dc2-4249-bff8-caef3204c5f7",
   "metadata": {},
   "source": [
    "9. Transfer learning.\n",
    "a. Create a new DNN that reuses all the pretrained hidden layers of the previous\n",
    "model, freezes them, and replaces the softmax output layer with a new one.\n",
    "b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time\n",
    "how long it takes. Despite this small number of examples, can you achieve\n",
    "high precision?\n",
    "c. Try caching the frozen layers, and train the model again: how much faster is it\n",
    "now?\n",
    "d. Try again reusing just four hidden layers instead of five. Can you achieve a\n",
    "higher precision?\n",
    "e. Now unfreeze the top two hidden layers and continue training: can you get\n",
    "the model to perform even better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e2819746-cd58-46f8-8023-24765f4b73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let the base model be the current model\n",
    "base_model = model\n",
    "# Clone the base model\n",
    "cloned_base = keras.models.clone_model(base_model)\n",
    "# Copy its weigts as well, since clone_model does not clone the weights\n",
    "cloned_base.set_weights(base_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a74b0223-5c2c-4738-86cb-f503e664bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Sequential model\n",
    "new_model = keras.models.Sequential()\n",
    "\n",
    "# Add the layers from the cloned base model (excluding the final output layer)\n",
    "for layer in cloned_base.layers[:-1]: # Loop through all layers except the last one\n",
    "    new_model.add(layer)\n",
    "    layer.trainable = False # Freeze the layers\n",
    "\n",
    "# Add a new outplut layer\n",
    "new_model.add(keras.layers.Dense(5, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "801d900f-5aef-479a-8a5f-ba58504d390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model: You must always compile after you freeze or unfreeze layers\n",
    "new_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c1a690-99dd-4aed-8cd4-8a0df1011718",
   "metadata": {},
   "source": [
    "b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3230b2ea-8591-470f-b8cf-481381f07000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26463\n",
      "2941\n",
      "4861\n",
      "26463\n",
      "2941\n",
      "4861\n"
     ]
    }
   ],
   "source": [
    "# We only need to get 100 of images per digit, let's get the data we are using\n",
    "print(len(X_train_59))\n",
    "print(len(X_val_59))\n",
    "print(len(X_test_59))\n",
    "print(len(y_train_59))\n",
    "print(len(y_val_59))\n",
    "print(len(y_test_59))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "94f52596-7342-4387-af3f-d7e9c6cee5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# So I am trying to save 100 images per digit in a new list\n",
    "X_train_100 = []\n",
    "y_train_100 = []\n",
    "\n",
    "# Create a dictionary to track the number of images per digit\n",
    "digit_counts = {5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for i, j in zip(X_train_59, y_train_59):\n",
    "    if digit_counts[j] < 100:\n",
    "        X_train_100.append(i)\n",
    "        y_train_100.append(j)\n",
    "        digit_counts[j] += 1\n",
    "\n",
    "# Change back to numpy array\n",
    "X_train_100 = np.array(X_train_100)\n",
    "y_train_100 = np.array(y_train_100)\n",
    "\n",
    "# Remap the y labels to be 0 to 4 cause of sparse\n",
    "y_train_59r = y_train_59 - 5\n",
    "y_test_59r = y_test_59 - 5\n",
    "y_val_59r = y_val_59 - 5\n",
    "y_train_100r = y_train_100 - 5\n",
    "\n",
    "\n",
    "\n",
    "# Check\n",
    "print(len(X_train_100))\n",
    "print(len(y_train_100)) \n",
    "print(type(X_train_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bb82800b-e928-4c93-8e48-05fa4cd1eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: 100, 6: 100, 7: 100, 8: 100, 9: 100}\n"
     ]
    }
   ],
   "source": [
    "print(digit_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "50f844c4-69b7-426b-a736-54b47d8e9e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - accuracy: 0.2052 - loss: 1.7402 - val_accuracy: 0.2781 - val_loss: 1.5794\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.2720 - loss: 1.6301 - val_accuracy: 0.3937 - val_loss: 1.5110\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.3586 - loss: 1.7440 - val_accuracy: 0.4539 - val_loss: 1.4524\n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.3754 - loss: 1.5819 - val_accuracy: 0.4726 - val_loss: 1.4083\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.4331 - loss: 1.3876 - val_accuracy: 0.4855 - val_loss: 1.3717\n",
      "Epoch 6/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 0.4684 - loss: 1.4279 - val_accuracy: 0.4947 - val_loss: 1.3382\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.4768 - loss: 1.3724 - val_accuracy: 0.5080 - val_loss: 1.3110\n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.4344 - loss: 1.4206 - val_accuracy: 0.5124 - val_loss: 1.2892\n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.4979 - loss: 1.7461 - val_accuracy: 0.5158 - val_loss: 1.2731\n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.4636 - loss: 1.3541 - val_accuracy: 0.5192 - val_loss: 1.2591\n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.4966 - loss: 1.2803 - val_accuracy: 0.5230 - val_loss: 1.2464\n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.4843 - loss: 1.2962 - val_accuracy: 0.5284 - val_loss: 1.2338\n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.4765 - loss: 1.3114 - val_accuracy: 0.5328 - val_loss: 1.2216\n",
      "Epoch 14/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.4860 - loss: 1.3078 - val_accuracy: 0.5349 - val_loss: 1.2112\n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.5360 - loss: 1.3077 - val_accuracy: 0.5393 - val_loss: 1.2013\n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.5076 - loss: 1.2862 - val_accuracy: 0.5406 - val_loss: 1.1937\n",
      "Epoch 17/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.4915 - loss: 1.2636 - val_accuracy: 0.5440 - val_loss: 1.1866\n",
      "Epoch 18/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.4881 - loss: 1.2903 - val_accuracy: 0.5498 - val_loss: 1.1801\n",
      "Epoch 19/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.5280 - loss: 1.1901 - val_accuracy: 0.5485 - val_loss: 1.1746\n",
      "Epoch 20/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.5004 - loss: 1.2143 - val_accuracy: 0.5536 - val_loss: 1.1708\n",
      "Epoch 21/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.5641 - loss: 1.1894 - val_accuracy: 0.5508 - val_loss: 1.1661\n",
      "Epoch 22/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.5204 - loss: 1.2213 - val_accuracy: 0.5559 - val_loss: 1.1617\n",
      "Epoch 23/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.5341 - loss: 1.1839 - val_accuracy: 0.5597 - val_loss: 1.1572\n",
      "Epoch 24/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.5293 - loss: 1.1820 - val_accuracy: 0.5672 - val_loss: 1.1530\n",
      "Epoch 25/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.4930 - loss: 1.3117 - val_accuracy: 0.5665 - val_loss: 1.1482\n",
      "Epoch 26/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.4999 - loss: 1.1706 - val_accuracy: 0.5695 - val_loss: 1.1440\n",
      "Epoch 27/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.5064 - loss: 1.2793 - val_accuracy: 0.5716 - val_loss: 1.1410\n",
      "Epoch 28/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.5147 - loss: 1.2104 - val_accuracy: 0.5709 - val_loss: 1.1378\n",
      "Epoch 29/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.5091 - loss: 1.2286 - val_accuracy: 0.5668 - val_loss: 1.1339\n",
      "Epoch 30/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.5471 - loss: 1.1677 - val_accuracy: 0.5746 - val_loss: 1.1307\n",
      "Epoch 31/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - accuracy: 0.5101 - loss: 1.2366 - val_accuracy: 0.5723 - val_loss: 1.1276\n",
      "Epoch 32/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.5207 - loss: 1.1894 - val_accuracy: 0.5723 - val_loss: 1.1240\n",
      "Epoch 33/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.5283 - loss: 1.2542 - val_accuracy: 0.5770 - val_loss: 1.1227\n",
      "Epoch 34/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.5610 - loss: 1.2071 - val_accuracy: 0.5784 - val_loss: 1.1206\n",
      "Epoch 35/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.5744 - loss: 1.1283 - val_accuracy: 0.5838 - val_loss: 1.1183\n",
      "Epoch 36/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.5379 - loss: 1.2720 - val_accuracy: 0.5862 - val_loss: 1.1162\n",
      "Epoch 37/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.5381 - loss: 1.1605 - val_accuracy: 0.5859 - val_loss: 1.1133\n",
      "Epoch 38/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.5483 - loss: 1.1641 - val_accuracy: 0.5865 - val_loss: 1.1112\n",
      "Epoch 39/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.5303 - loss: 1.1627 - val_accuracy: 0.5855 - val_loss: 1.1094\n",
      "Epoch 40/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.5062 - loss: 1.1830 - val_accuracy: 0.5923 - val_loss: 1.1074\n",
      "Epoch 41/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.5118 - loss: 1.1456 - val_accuracy: 0.5923 - val_loss: 1.1058\n",
      "Epoch 42/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.5218 - loss: 1.1679 - val_accuracy: 0.5896 - val_loss: 1.1036\n",
      "Epoch 43/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.5727 - loss: 1.1584 - val_accuracy: 0.5947 - val_loss: 1.1015\n",
      "Epoch 44/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.5611 - loss: 1.1993 - val_accuracy: 0.5930 - val_loss: 1.0999\n",
      "Epoch 45/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.5642 - loss: 1.1901 - val_accuracy: 0.5920 - val_loss: 1.0977\n",
      "Epoch 46/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.5691 - loss: 1.1950 - val_accuracy: 0.5937 - val_loss: 1.0959\n",
      "Epoch 47/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.5721 - loss: 1.1182 - val_accuracy: 0.5937 - val_loss: 1.0943\n",
      "Epoch 48/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.5307 - loss: 1.1726 - val_accuracy: 0.5913 - val_loss: 1.0929\n",
      "Epoch 49/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.5500 - loss: 1.1523 - val_accuracy: 0.5886 - val_loss: 1.0916\n",
      "Epoch 50/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5940 - loss: 1.0685 - val_accuracy: 0.5855 - val_loss: 1.0902\n",
      "Epoch 51/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.5577 - loss: 1.1465 - val_accuracy: 0.5923 - val_loss: 1.0897\n",
      "Epoch 52/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.6156 - loss: 1.0501 - val_accuracy: 0.5940 - val_loss: 1.0881\n",
      "Epoch 53/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.5644 - loss: 1.1623 - val_accuracy: 0.5964 - val_loss: 1.0868\n",
      "Epoch 54/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.5561 - loss: 1.1744 - val_accuracy: 0.5974 - val_loss: 1.0860\n",
      "Epoch 55/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.5439 - loss: 1.5555 - val_accuracy: 0.5910 - val_loss: 1.0855\n",
      "Epoch 56/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.5116 - loss: 1.2067 - val_accuracy: 0.5862 - val_loss: 1.0861\n",
      "Epoch 57/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.5191 - loss: 1.1721 - val_accuracy: 0.5842 - val_loss: 1.0860\n",
      "Epoch 58/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.5872 - loss: 1.0932 - val_accuracy: 0.5876 - val_loss: 1.0840\n",
      "Epoch 59/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.5315 - loss: 1.1373 - val_accuracy: 0.5865 - val_loss: 1.0827\n",
      "Epoch 60/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.5344 - loss: 1.1476 - val_accuracy: 0.5981 - val_loss: 1.0822\n",
      "Epoch 61/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.5817 - loss: 1.0488 - val_accuracy: 0.6052 - val_loss: 1.0812\n",
      "Epoch 62/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.5759 - loss: 1.1287 - val_accuracy: 0.6015 - val_loss: 1.0797\n",
      "Epoch 63/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.6109 - loss: 1.1197 - val_accuracy: 0.5981 - val_loss: 1.0788\n",
      "Epoch 64/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.5419 - loss: 1.1558 - val_accuracy: 0.5974 - val_loss: 1.0774\n",
      "Epoch 65/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.5517 - loss: 1.1519 - val_accuracy: 0.5916 - val_loss: 1.0752\n",
      "Epoch 66/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.5362 - loss: 1.1280 - val_accuracy: 0.5869 - val_loss: 1.0740\n",
      "Epoch 67/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5497 - loss: 1.0599 - val_accuracy: 0.5872 - val_loss: 1.0731\n",
      "Epoch 68/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.5451 - loss: 1.1238 - val_accuracy: 0.5862 - val_loss: 1.0724\n",
      "Epoch 69/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5349 - loss: 1.1235 - val_accuracy: 0.5930 - val_loss: 1.0723\n",
      "Epoch 70/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.5121 - loss: 1.1383 - val_accuracy: 0.5923 - val_loss: 1.0720\n",
      "Epoch 71/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.5291 - loss: 1.1280 - val_accuracy: 0.5913 - val_loss: 1.0716\n",
      "Epoch 72/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.5651 - loss: 1.1831 - val_accuracy: 0.5950 - val_loss: 1.0709\n",
      "Epoch 73/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.6039 - loss: 1.0777 - val_accuracy: 0.5981 - val_loss: 1.0700\n",
      "Epoch 74/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.5573 - loss: 1.1076 - val_accuracy: 0.5954 - val_loss: 1.0690\n",
      "Epoch 75/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.5544 - loss: 1.1605 - val_accuracy: 0.5988 - val_loss: 1.0680\n",
      "Epoch 76/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.5449 - loss: 1.1231 - val_accuracy: 0.5940 - val_loss: 1.0674\n",
      "Epoch 77/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.5170 - loss: 1.2035 - val_accuracy: 0.5964 - val_loss: 1.0669\n",
      "Epoch 78/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.5142 - loss: 1.1898 - val_accuracy: 0.5991 - val_loss: 1.0659\n",
      "Epoch 79/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 119ms/step - accuracy: 0.5419 - loss: 1.1123 - val_accuracy: 0.5954 - val_loss: 1.0652\n",
      "Epoch 80/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.5541 - loss: 1.0849 - val_accuracy: 0.5974 - val_loss: 1.0644\n",
      "Epoch 81/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.5368 - loss: 1.0801 - val_accuracy: 0.6035 - val_loss: 1.0635\n",
      "Epoch 82/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5532 - loss: 1.1161 - val_accuracy: 0.6022 - val_loss: 1.0629\n",
      "Epoch 83/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.5463 - loss: 1.0905 - val_accuracy: 0.6022 - val_loss: 1.0622\n",
      "Epoch 84/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.5612 - loss: 1.1387 - val_accuracy: 0.5978 - val_loss: 1.0618\n",
      "Epoch 85/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5849 - loss: 1.0940 - val_accuracy: 0.6042 - val_loss: 1.0614\n",
      "Epoch 86/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.5719 - loss: 1.1072 - val_accuracy: 0.5995 - val_loss: 1.0612\n",
      "Epoch 87/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.5464 - loss: 1.1121 - val_accuracy: 0.5957 - val_loss: 1.0605\n",
      "Epoch 88/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.5164 - loss: 1.1237 - val_accuracy: 0.6029 - val_loss: 1.0597\n",
      "Epoch 89/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.5430 - loss: 1.1433 - val_accuracy: 0.6029 - val_loss: 1.0598\n",
      "Epoch 90/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - accuracy: 0.5909 - loss: 1.0620 - val_accuracy: 0.6073 - val_loss: 1.0584\n",
      "Epoch 91/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5429 - loss: 1.1000 - val_accuracy: 0.6069 - val_loss: 1.0577\n",
      "Epoch 92/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.5378 - loss: 1.1359 - val_accuracy: 0.6035 - val_loss: 1.0582\n",
      "Epoch 93/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.5725 - loss: 1.1741 - val_accuracy: 0.6046 - val_loss: 1.0561\n",
      "Epoch 94/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.5803 - loss: 1.0941 - val_accuracy: 0.6069 - val_loss: 1.0551\n",
      "Epoch 95/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.5856 - loss: 1.0641 - val_accuracy: 0.6090 - val_loss: 1.0544\n",
      "Epoch 96/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.5605 - loss: 1.1728 - val_accuracy: 0.6120 - val_loss: 1.0548\n",
      "Epoch 97/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.5653 - loss: 1.2208 - val_accuracy: 0.6100 - val_loss: 1.0529\n",
      "Epoch 98/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.5626 - loss: 1.0954 - val_accuracy: 0.6134 - val_loss: 1.0515\n",
      "Epoch 99/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5798 - loss: 1.1244 - val_accuracy: 0.6097 - val_loss: 1.0506\n",
      "Epoch 100/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step - accuracy: 0.5790 - loss: 1.1083 - val_accuracy: 0.6103 - val_loss: 1.0508\n",
      "Time it used to train: 200.48957514762878\n"
     ]
    }
   ],
   "source": [
    "# Now we are training with this and time how long it takes to train\n",
    "import time\n",
    "\n",
    "# Callbacks\n",
    "my_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True), # so that when it stops improving after 10 epochs, it would stop\n",
    "    keras.callbacks.ModelCheckpoint(\"mnist_59.keras\", save_best_only=True), # this saves the model at regular intervals but at the epochs with best results\n",
    "    keras.callbacks.TensorBoard(\"logs/fit/59\")\n",
    "]\n",
    "   \n",
    "\n",
    "start_time = time.time()\n",
    "# Fit\n",
    "history = new_model.fit(x=X_train_100, y=y_train_100r, epochs=100, validation_data=(X_val_59, y_val_59r), callbacks=my_callbacks)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time it used to train: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4ee8ae0-54c0-4980-af11-a8df05ff8726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 26852), started 2:23:15 ago. (Use '!kill 26852' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1f72ba3f549a9deb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1f72ba3f549a9deb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs/fit/59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd6472-1516-4c4a-bc0b-4d313f0f7d21",
   "metadata": {},
   "source": [
    "c. Try caching the frozen layers, and train the model again: how much faster is it\n",
    "now?\n",
    "Tensorflow already does this automatically so idk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6cf7d6-2595-46c0-b311-11688b5485cf",
   "metadata": {},
   "source": [
    "d. Try again reusing just four hidden layers instead of five. Can you achieve a\n",
    "higher precision?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "991dd51a-5922-49ae-8ff6-6ce188907cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_58\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_58\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_306 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">78,400</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_307 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_308 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_309 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_310 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_316 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">505</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten_51 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │           \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_306 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m78,400\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_307 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_308 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_309 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_310 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_9 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_316 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m505\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,053</span> (488.49 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m125,053\u001b[0m (488.49 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">505</span> (1.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m505\u001b[0m (1.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">123,536</span> (482.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m123,536\u001b[0m (482.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,012</span> (3.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,012\u001b[0m (3.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8f6bcb7b-035a-4bf9-a872-77bdc1c86d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(new_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fb3d9e99-d635-4d9c-bd98-b8ab2a9d0c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Flatten name=flatten_51, built=True>,\n",
       " <BatchNormalization name=batch_normalization_6, built=True>,\n",
       " <Dropout name=dropout, built=True>,\n",
       " <Dense name=dense_306, built=True>,\n",
       " <Activation name=activation_5, built=True>,\n",
       " <BatchNormalization name=batch_normalization_7, built=True>,\n",
       " <Dropout name=dropout_1, built=True>,\n",
       " <Dense name=dense_307, built=True>,\n",
       " <Activation name=activation_6, built=True>,\n",
       " <BatchNormalization name=batch_normalization_8, built=True>,\n",
       " <Dropout name=dropout_2, built=True>,\n",
       " <Dense name=dense_308, built=True>,\n",
       " <Activation name=activation_7, built=True>,\n",
       " <BatchNormalization name=batch_normalization_9, built=True>,\n",
       " <Dropout name=dropout_3, built=True>,\n",
       " <Dense name=dense_309, built=True>,\n",
       " <Activation name=activation_8, built=True>,\n",
       " <BatchNormalization name=batch_normalization_10, built=True>,\n",
       " <Dropout name=dropout_4, built=True>,\n",
       " <Dense name=dense_310, built=True>,\n",
       " <Activation name=activation_9, built=True>,\n",
       " <BatchNormalization name=batch_normalization_11, built=True>,\n",
       " <Dropout name=dropout_5, built=True>,\n",
       " <Dense name=dense_316, built=True>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7ce922c3-f800-4295-9497-e9810f15964d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_61\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_61\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_306 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">78,400</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_307 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_308 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_309 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_316 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">505</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten_51 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │           \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_306 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m78,400\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_307 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_308 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_309 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_316 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m505\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,641</span> (443.91 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m113,641\u001b[0m (443.91 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">505</span> (1.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m505\u001b[0m (1.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">113,136</span> (441.94 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m113,136\u001b[0m (441.94 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take out one hidden layer from the model,\n",
    "# i will also take out the corresponding BN, Dropout and activation layers for that \"one layer\"\n",
    "# Create a new model\n",
    "old_model = new_model\n",
    "new_model = keras.models.Sequential()\n",
    "#add layers you want to keep\n",
    "for index in range(len(old_model.layers)):\n",
    "    if index not in {17, 18, 19, 20}:\n",
    "        new_model.add(old_model.layers[index])\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "30160989-5dc9-469e-897b-80c10a5e239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(new_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "02b64b1c-17a4-462a-b026-b64ac51fca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compile and train again\n",
    "new_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9744a43c-eb72-453e-be78-7096192def9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - accuracy: 0.2170 - loss: 488.9446 - val_accuracy: 0.2520 - val_loss: 395.9422\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.2672 - loss: 430.2499 - val_accuracy: 0.2737 - val_loss: 345.8837\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.2796 - loss: 388.3739 - val_accuracy: 0.2815 - val_loss: 295.4886\n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.2411 - loss: 335.0635 - val_accuracy: 0.2894 - val_loss: 249.5870\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.2813 - loss: 259.3115 - val_accuracy: 0.2914 - val_loss: 208.3234\n",
      "Epoch 6/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.2631 - loss: 231.1711 - val_accuracy: 0.2975 - val_loss: 169.2914\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.3108 - loss: 203.7036 - val_accuracy: 0.3070 - val_loss: 129.8407\n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.3008 - loss: 163.6573 - val_accuracy: 0.3370 - val_loss: 97.0812\n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.3358 - loss: 126.3503 - val_accuracy: 0.3591 - val_loss: 75.1486\n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.3383 - loss: 122.4728 - val_accuracy: 0.3546 - val_loss: 67.7527\n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.3105 - loss: 123.1045 - val_accuracy: 0.3461 - val_loss: 60.6981\n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.3001 - loss: 102.0406 - val_accuracy: 0.3472 - val_loss: 57.5106\n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.3031 - loss: 112.5721 - val_accuracy: 0.3744 - val_loss: 47.6723\n",
      "Epoch 14/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.3536 - loss: 89.5069 - val_accuracy: 0.3812 - val_loss: 40.9319\n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.3645 - loss: 66.0726 - val_accuracy: 0.3791 - val_loss: 35.4608\n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.3214 - loss: 80.0763 - val_accuracy: 0.3706 - val_loss: 30.0480\n",
      "Epoch 17/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.3459 - loss: 71.7564 - val_accuracy: 0.3761 - val_loss: 25.3027\n",
      "Epoch 18/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.3095 - loss: 63.2685 - val_accuracy: 0.4039 - val_loss: 20.9653\n",
      "Epoch 19/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.3574 - loss: 51.3477 - val_accuracy: 0.4039 - val_loss: 18.0883\n",
      "Epoch 20/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.3443 - loss: 50.0554 - val_accuracy: 0.4492 - val_loss: 13.6767\n",
      "Epoch 21/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.3843 - loss: 40.2441 - val_accuracy: 0.4978 - val_loss: 10.7803\n",
      "Epoch 22/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.3909 - loss: 36.4199 - val_accuracy: 0.5366 - val_loss: 8.9786\n",
      "Epoch 23/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.4229 - loss: 24.5038 - val_accuracy: 0.5094 - val_loss: 8.8632\n",
      "Epoch 24/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.4346 - loss: 27.6559 - val_accuracy: 0.5396 - val_loss: 6.9844\n",
      "Epoch 25/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.4253 - loss: 20.8652 - val_accuracy: 0.5362 - val_loss: 7.3237\n",
      "Epoch 26/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.4499 - loss: 22.3143 - val_accuracy: 0.5437 - val_loss: 7.0640\n",
      "Epoch 27/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.4205 - loss: 22.8434 - val_accuracy: 0.5692 - val_loss: 5.5652\n",
      "Epoch 28/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.4360 - loss: 16.7497 - val_accuracy: 0.5828 - val_loss: 5.7151\n",
      "Epoch 29/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.4395 - loss: 12.9017 - val_accuracy: 0.5549 - val_loss: 5.7527\n",
      "Epoch 30/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.4896 - loss: 10.2929 - val_accuracy: 0.5886 - val_loss: 4.4892\n",
      "Epoch 31/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.4092 - loss: 13.9546 - val_accuracy: 0.5913 - val_loss: 4.2483\n",
      "Epoch 32/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.4577 - loss: 9.6628 - val_accuracy: 0.5515 - val_loss: 4.8456\n",
      "Epoch 33/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.4837 - loss: 8.3829 - val_accuracy: 0.6080 - val_loss: 4.7514\n",
      "Epoch 34/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.4736 - loss: 10.0870 - val_accuracy: 0.5971 - val_loss: 3.6430\n",
      "Epoch 35/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.4772 - loss: 7.2751 - val_accuracy: 0.5848 - val_loss: 3.4672\n",
      "Epoch 36/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.4487 - loss: 7.9589 - val_accuracy: 0.5621 - val_loss: 3.8622\n",
      "Epoch 37/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.4815 - loss: 7.4344 - val_accuracy: 0.5797 - val_loss: 3.9196\n",
      "Epoch 38/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.4742 - loss: 7.2791 - val_accuracy: 0.5440 - val_loss: 3.4667\n",
      "Epoch 39/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.4540 - loss: 6.4926 - val_accuracy: 0.5644 - val_loss: 4.5585\n",
      "Epoch 40/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 102ms/step - accuracy: 0.5134 - loss: 6.3112 - val_accuracy: 0.5644 - val_loss: 3.6746\n",
      "Epoch 41/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.4465 - loss: 7.5138 - val_accuracy: 0.5655 - val_loss: 3.1757\n",
      "Epoch 42/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 141ms/step - accuracy: 0.4724 - loss: 6.9710 - val_accuracy: 0.5505 - val_loss: 3.4436\n",
      "Epoch 43/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.4165 - loss: 8.6096 - val_accuracy: 0.5386 - val_loss: 5.4431\n",
      "Epoch 44/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.4082 - loss: 7.1546 - val_accuracy: 0.5444 - val_loss: 3.6052\n",
      "Epoch 45/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.4441 - loss: 5.7976 - val_accuracy: 0.6209 - val_loss: 3.2209\n",
      "Epoch 46/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.4329 - loss: 6.2898 - val_accuracy: 0.5794 - val_loss: 3.5353\n",
      "Epoch 47/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.5086 - loss: 5.3781 - val_accuracy: 0.5828 - val_loss: 3.1255\n",
      "Epoch 48/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.4598 - loss: 5.5882 - val_accuracy: 0.5746 - val_loss: 2.8947\n",
      "Epoch 49/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.4834 - loss: 4.6286 - val_accuracy: 0.5267 - val_loss: 2.8772\n",
      "Epoch 50/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.4651 - loss: 4.9431 - val_accuracy: 0.6083 - val_loss: 2.9307\n",
      "Epoch 51/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - accuracy: 0.5481 - loss: 4.2884 - val_accuracy: 0.5726 - val_loss: 2.5510\n",
      "Epoch 52/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.4712 - loss: 6.5713 - val_accuracy: 0.4917 - val_loss: 3.2314\n",
      "Epoch 53/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.4320 - loss: 4.4341 - val_accuracy: 0.5923 - val_loss: 2.8812\n",
      "Epoch 54/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.5442 - loss: 5.3761 - val_accuracy: 0.5855 - val_loss: 2.9227\n",
      "Epoch 55/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - accuracy: 0.5702 - loss: 3.4214 - val_accuracy: 0.5852 - val_loss: 2.3346\n",
      "Epoch 56/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.5063 - loss: 3.6690 - val_accuracy: 0.5359 - val_loss: 2.6596\n",
      "Epoch 57/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 108ms/step - accuracy: 0.4443 - loss: 4.7778 - val_accuracy: 0.5576 - val_loss: 3.3188\n",
      "Epoch 58/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 123ms/step - accuracy: 0.5177 - loss: 4.9487 - val_accuracy: 0.5155 - val_loss: 2.6857\n",
      "Epoch 59/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.5025 - loss: 3.6218 - val_accuracy: 0.6049 - val_loss: 2.5554\n",
      "Epoch 60/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.4776 - loss: 3.4328 - val_accuracy: 0.6032 - val_loss: 2.2313\n",
      "Epoch 61/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - accuracy: 0.4859 - loss: 3.3387 - val_accuracy: 0.5845 - val_loss: 2.2393\n",
      "Epoch 62/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 132ms/step - accuracy: 0.4771 - loss: 3.3474 - val_accuracy: 0.5876 - val_loss: 2.1218\n",
      "Epoch 63/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 137ms/step - accuracy: 0.5416 - loss: 3.8497 - val_accuracy: 0.5692 - val_loss: 2.0044\n",
      "Epoch 64/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.5126 - loss: 3.0730 - val_accuracy: 0.5219 - val_loss: 2.1123\n",
      "Epoch 65/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.5002 - loss: 3.9631 - val_accuracy: 0.5760 - val_loss: 1.8995\n",
      "Epoch 66/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.4953 - loss: 2.7241 - val_accuracy: 0.5155 - val_loss: 2.2323\n",
      "Epoch 67/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.5047 - loss: 2.8078 - val_accuracy: 0.5447 - val_loss: 2.1887\n",
      "Epoch 68/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 109ms/step - accuracy: 0.4742 - loss: 3.0783 - val_accuracy: 0.5607 - val_loss: 2.2940\n",
      "Epoch 69/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.4963 - loss: 2.5047 - val_accuracy: 0.5872 - val_loss: 1.8205\n",
      "Epoch 70/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - accuracy: 0.4949 - loss: 2.2809 - val_accuracy: 0.4502 - val_loss: 2.8637\n",
      "Epoch 71/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.4582 - loss: 3.3969 - val_accuracy: 0.4383 - val_loss: 2.5246\n",
      "Epoch 72/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.4445 - loss: 3.0746 - val_accuracy: 0.5610 - val_loss: 1.9320\n",
      "Epoch 73/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.4934 - loss: 2.2293 - val_accuracy: 0.5512 - val_loss: 1.9117\n",
      "Epoch 74/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.4604 - loss: 3.8330 - val_accuracy: 0.5614 - val_loss: 2.2508\n",
      "Epoch 75/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.4442 - loss: 3.0645 - val_accuracy: 0.5274 - val_loss: 1.9937\n",
      "Epoch 76/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - accuracy: 0.5164 - loss: 2.9208 - val_accuracy: 0.5053 - val_loss: 2.0095\n",
      "Epoch 77/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.4348 - loss: 2.2549 - val_accuracy: 0.5910 - val_loss: 1.4903\n",
      "Epoch 78/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.4453 - loss: 2.2882 - val_accuracy: 0.4866 - val_loss: 2.0592\n",
      "Epoch 79/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 0.4669 - loss: 2.8877 - val_accuracy: 0.5784 - val_loss: 1.5357\n",
      "Epoch 80/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.5041 - loss: 2.2589 - val_accuracy: 0.4961 - val_loss: 2.2823\n",
      "Epoch 81/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.4710 - loss: 2.9020 - val_accuracy: 0.4260 - val_loss: 2.9814\n",
      "Epoch 82/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.3978 - loss: 3.1577 - val_accuracy: 0.5349 - val_loss: 1.6271\n",
      "Epoch 83/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.5200 - loss: 2.0271 - val_accuracy: 0.5002 - val_loss: 1.6005\n",
      "Epoch 84/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.4775 - loss: 2.3105 - val_accuracy: 0.5185 - val_loss: 1.7493\n",
      "Epoch 85/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.4655 - loss: 2.4274 - val_accuracy: 0.5920 - val_loss: 1.3239\n",
      "Epoch 86/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.4446 - loss: 3.3812 - val_accuracy: 0.5209 - val_loss: 2.5466\n",
      "Epoch 87/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.5295 - loss: 1.8834 - val_accuracy: 0.5672 - val_loss: 1.5956\n",
      "Epoch 88/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - accuracy: 0.4705 - loss: 2.0247 - val_accuracy: 0.5723 - val_loss: 1.3148\n",
      "Epoch 89/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5228 - loss: 1.4705 - val_accuracy: 0.6042 - val_loss: 1.1831\n",
      "Epoch 90/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.4800 - loss: 1.5077 - val_accuracy: 0.5811 - val_loss: 1.1821\n",
      "Epoch 91/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.5245 - loss: 1.6948 - val_accuracy: 0.5984 - val_loss: 1.2325\n",
      "Epoch 92/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.4657 - loss: 1.6636 - val_accuracy: 0.5719 - val_loss: 1.1486\n",
      "Epoch 93/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.4581 - loss: 1.8348 - val_accuracy: 0.5121 - val_loss: 1.7126\n",
      "Epoch 94/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.4516 - loss: 2.3006 - val_accuracy: 0.5066 - val_loss: 2.7966\n",
      "Epoch 95/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.4997 - loss: 2.4068 - val_accuracy: 0.5495 - val_loss: 1.5015\n",
      "Epoch 96/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.4313 - loss: 2.6224 - val_accuracy: 0.5260 - val_loss: 2.2217\n",
      "Epoch 97/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - accuracy: 0.4881 - loss: 2.2739 - val_accuracy: 0.5335 - val_loss: 1.5043\n",
      "Epoch 98/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - accuracy: 0.4822 - loss: 1.7492 - val_accuracy: 0.5134 - val_loss: 1.8463\n",
      "Epoch 99/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.4229 - loss: 2.5151 - val_accuracy: 0.5226 - val_loss: 2.2576\n",
      "Epoch 100/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.4547 - loss: 2.2830 - val_accuracy: 0.5294 - val_loss: 2.4270\n",
      "Time it used to train: 195.04540491104126\n"
     ]
    }
   ],
   "source": [
    "#Now we are training with this and time how long it takes to train\n",
    "import time\n",
    "\n",
    "# Callbacks\n",
    "my_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True), # so that when it stops improving after 10 epochs, it would stop\n",
    "    keras.callbacks.ModelCheckpoint(\"mnist_59.keras\", save_best_only=True), # this saves the model at regular intervals but at the epochs with best results\n",
    "    keras.callbacks.TensorBoard(\"logs/fit/59\")\n",
    "]\n",
    "   \n",
    "\n",
    "start_time = time.time()\n",
    "# Fit\n",
    "history = new_model.fit(x=X_train_100, y=y_train_100r, epochs=100, validation_data=(X_val_59, y_val_59r), callbacks=my_callbacks)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time it used to train: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2c84c905-55b5-47b1-95c6-91c6d2da313c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 26852), started 2:41:23 ago. (Use '!kill 26852' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a0d09021795745a2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a0d09021795745a2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs/fit/59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f00b86-7b9c-40c6-8a63-1d48a3f01327",
   "metadata": {},
   "source": [
    "e. Now unfreeze the top two hidden layers and continue training: can you get\n",
    "the model to perform even better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ab62a10-796e-491b-a574-db32cca8a8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_61\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_61\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,136</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_306 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">78,400</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_307 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_308 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_309 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_316 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">505</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ flatten_51 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │           \u001b[38;5;34m3,136\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_306 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m78,400\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_307 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_308 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_309 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m10,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │             \u001b[38;5;34m400\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_316 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m505\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">114,653</span> (447.87 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m114,653\u001b[0m (447.87 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,473</span> (314.35 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,473\u001b[0m (314.35 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">33,168</span> (129.56 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m33,168\u001b[0m (129.56 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,012</span> (3.96 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,012\u001b[0m (3.96 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unfreeze te top 2 layers\n",
    "for layer in new_model.layers[:5]:\n",
    "    layer.trainable = True\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a074eca0-77d7-4a98-986e-2671dc8843f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compile and train again\n",
    "new_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "65b0b4f3-5b2e-4c9d-83cc-35cb0205e0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - accuracy: 0.4431 - loss: 1.9344 - val_accuracy: 0.5406 - val_loss: 1.8473\n",
      "Epoch 2/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - accuracy: 0.4847 - loss: 1.7634 - val_accuracy: 0.5209 - val_loss: 1.9847\n",
      "Epoch 3/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.4445 - loss: 1.6019 - val_accuracy: 0.4128 - val_loss: 1.7073\n",
      "Epoch 4/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.4289 - loss: 1.6673 - val_accuracy: 0.5400 - val_loss: 1.2428\n",
      "Epoch 5/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.4382 - loss: 1.5554 - val_accuracy: 0.5342 - val_loss: 1.3785\n",
      "Epoch 6/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - accuracy: 0.4775 - loss: 1.3549 - val_accuracy: 0.6012 - val_loss: 1.1158\n",
      "Epoch 7/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.5378 - loss: 1.2346 - val_accuracy: 0.5794 - val_loss: 1.2410\n",
      "Epoch 8/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.4814 - loss: 1.5308 - val_accuracy: 0.5607 - val_loss: 1.1618\n",
      "Epoch 9/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 80ms/step - accuracy: 0.5192 - loss: 1.3973 - val_accuracy: 0.5461 - val_loss: 1.1733\n",
      "Epoch 10/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.4676 - loss: 1.4163 - val_accuracy: 0.5308 - val_loss: 1.3477\n",
      "Epoch 11/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - accuracy: 0.4850 - loss: 1.5167 - val_accuracy: 0.4954 - val_loss: 1.3931\n",
      "Epoch 12/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.5150 - loss: 1.3052 - val_accuracy: 0.5814 - val_loss: 1.1883\n",
      "Epoch 13/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.4962 - loss: 1.5266 - val_accuracy: 0.5121 - val_loss: 1.3203\n",
      "Epoch 14/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.4554 - loss: 1.4546 - val_accuracy: 0.6059 - val_loss: 1.1464\n",
      "Epoch 15/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.5089 - loss: 1.3471 - val_accuracy: 0.5566 - val_loss: 1.1827\n",
      "Epoch 16/100\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.5166 - loss: 1.3612 - val_accuracy: 0.5264 - val_loss: 1.3563\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "my_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True), # so that when it stops improving after 10 epochs, it would stop\n",
    "    keras.callbacks.ModelCheckpoint(\"mnist_59.keras\", save_best_only=True), # this saves the model at regular intervals but at the epochs with best results\n",
    "    keras.callbacks.TensorBoard(\"logs/fit/59\")\n",
    "]\n",
    "   \n",
    "\n",
    "start_time = time.time()\n",
    "# Fit\n",
    "history = new_model.fit(x=X_train_100, y=y_train_100r, epochs=100, validation_data=(X_val_59, y_val_59r), callbacks=my_callbacks)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63b9fb-836f-43f5-94c7-41d05d37791a",
   "metadata": {},
   "source": [
    "10. Pretraining on an auxiliary task.\n",
    "a. In this exercise you will build a DNN that compares two MNIST digit images\n",
    "and predicts whether they represent the same digit or not. Then you will reuse\n",
    "the lower layers of this network to train an MNIST classifier using very little\n",
    "training data. Start by building two DNNs (let’s call them DNN A and B), both\n",
    "similar to the one you built earlier but without the output layer: each DNN\n",
    "should have five hidden layers of 100 neurons each, He initialization, and ELU\n",
    "activation. Next, add one more hidden layer with 10 units on top of both\n",
    "DNNs. To do this, you should use a keras.layers.Concatenate layer to con‐\n",
    "catenate the outputs of both DNNs for each instance, then feed the result to\n",
    "the hidden layer. Finally, add an output layer with a single neuron using the\n",
    "logistic activation function.\n",
    "b. Split the MNIST training set in two sets: split #1 should containing 55,000\n",
    "images, and split #2 should contain contain 5,000 images. Create a function\n",
    "that generates a training batch where each instance is a pair of MNIST images\n",
    "picked from split #1. Half of the training instances should be pairs of images\n",
    "that belong to the same class, while the other half should be images from dif‐\n",
    "ferent classes. For each pair, the training label should be 0 if the images are\n",
    "from the same class, or 1 if they are from different classes.\n",
    "c. Train the DNN on this training set. For each image pair, you can simultane‐\n",
    "ously feed the first image to DNN A and the second image to DNN B. The\n",
    "whole network will gradually learn to tell whether two images belong to the\n",
    "same class or not.\n",
    "d. Now create a new DNN by reusing and freezing the hidden layers of DNN A\n",
    "and adding a softmax output layer on top with 10 neurons. Train this network\n",
    "on split #2 and see if you can achieve high performance despite having only\n",
    "500 images per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "34c9f9b0-b722-463d-b58a-4c0883d521bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by building the models with 5 hidden layers, 100 neurons, he ini and elu activation\n",
    "# Use functional API\n",
    "# DNN-A\n",
    "input_a = keras.layers.Input(shape=[28, 28])\n",
    "flatten_a = keras.layers.Flatten()(input_a)\n",
    "hidden1a = keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\")(flatten_a)\n",
    "hidden2a = keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\")(hidden1a)\n",
    "hidden3a = keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\")(hidden2a)\n",
    "hidden4a = keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\")(hidden3a)\n",
    "hidden5a = keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\")(hidden4a)\n",
    "\n",
    "# DNN-B\n",
    "input_b = keras.layers.Input(shape=[28, 28])\n",
    "flatten_b = keras.layers.Flatten()(input_b)\n",
    "hidden1b = keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\")(flatten_a)\n",
    "hidden2b = keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\")(hidden1b)\n",
    "hidden3b = keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\")(hidden2b)\n",
    "hidden4b = keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\")(hidden3b)\n",
    "hidden5b = keras.layers.Dense(100, kernel_initializer=\"he_normal\", activation=\"elu\")(hidden4b)\n",
    "\n",
    "# Concatenate DNN A and B\n",
    "concat = keras.layers.Concatenate()([hidden5a, hidden5b])\n",
    "\n",
    "# Add a new hidden layer\n",
    "hidden6 = keras.layers.Dense(10)(concat)\n",
    "\n",
    "# Output layer\n",
    "output = keras.layers.Dense(1, activation=\"sigmoid\")(hidden6)\n",
    "\n",
    "# Build the model\n",
    "f_model = keras.models.Model(inputs=[input_a, input_b], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f146a-ea24-4f20-a141-ec8f906aec91",
   "metadata": {},
   "source": [
    "b. Split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from dif‐ ferent classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0d87bf25-0fb2-4526-bbd0-5309b359d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_full))\n",
    "print(len(y_train_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "26cee6f6-a056-4b7b-935e-5ef0ce2a2ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000 5000\n"
     ]
    }
   ],
   "source": [
    "X_train_1 = X_train_full[:55000]\n",
    "X_train_2 = X_train_full[55000:]\n",
    "y_train_1 = y_train_full[:55000]\n",
    "y_train_2 = y_train_full[55000:]\n",
    "print(len(X_train_1), len(X_train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7bd6e7a2-e21b-40f6-9079-bc4869cb448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd0177-591e-43ef-94e3-952abba0ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the sorted indices\n",
    "sorted_indices = np.argsort(y_train)\n",
    "\n",
    "# then i wil use it to split\n",
    "sorted_right = sorted_indices[1::2]\n",
    "sorted_left = sorted_indices[2::2]\n",
    "# i wanna add the first index to the second one\n",
    "sorted_left = np.append(sorted_left, sorted_indices[0])\n",
    "\n",
    "\n",
    "# now create the pair\n",
    "pair_indices = []\n",
    "for pair in zip (sorted_right, sorted_left):\n",
    "    pair_indices.append(pair)\n",
    "\n",
    "# This way because its sorted the pairs would be half the same and half different\n",
    "\n",
    "# change it back to a np.array\n",
    "pair_indices = np.array(pair_indices)\n",
    "\n",
    "# now get the corresponding data of the pair\n",
    "y_train1_pair = y_train_1[[pair_indices]\n",
    "x_train1_pair = X_train_1[pair_indices]\n",
    "\n",
    "# Now get the new labels for x_train1_pair. if the same number = 0, different numbers = 1\n",
    "new_ytrain1 = []\n",
    "for i, j in y_train1_pair:\n",
    "    if i == j:\n",
    "        new_ytrain1.append(0)\n",
    "    else:\n",
    "        new_ytrain1.append(1)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
